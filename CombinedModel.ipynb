{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# to see all the comands result in a single kernal \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# to increase no. of rows and column visibility in outputs\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', 2000)\n",
    "\n",
    "#To ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   InvoiceNo  StockCode  Description  Quantity         InvoiceDate  UnitPrice  CustomerID  Country\n",
      "0       6141       1583          144         3 2011-05-06 16:54:00       3.75     14056.0       35\n",
      "1       6349       1300         3682         6 2011-05-11 07:35:00       1.95     13098.0       35\n",
      "2      16783       2178         1939         4 2011-11-20 13:20:00       5.95     15044.0       35\n",
      "3      16971       2115         2983         1 2011-11-22 12:07:00       0.83     15525.0       35\n",
      "4       6080       1210         2886        12 2011-05-06 09:00:00       1.65     13952.0       35\n",
      "284780\n",
      "278579\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Train.csv\", parse_dates = [\"InvoiceDate\"])\n",
    "print(train_df.head())\n",
    "\n",
    "train_df_org = pd.read_csv(\"Train.csv\", parse_dates = [\"InvoiceDate\"])\n",
    "\n",
    "# Converting datetime to date objects.\n",
    "train_df['InvoiceDate_org'] = train_df['InvoiceDate']\n",
    "train_df['InvoiceDate'] = train_df['InvoiceDate'].apply(lambda x:x.date())\n",
    "train_df['CustomerID'] = train_df['CustomerID'].apply(lambda x:str(int(float(x))))\n",
    "\n",
    "train_df_org['CustomerID'] = train_df['CustomerID'].apply(lambda x:str(int(float(x))))\n",
    "\n",
    "train_df['Quantity_org'] = train_df['Quantity']\n",
    "train_df['Quantity'] = train_df['Quantity'].apply(abs)\n",
    "\n",
    "train_df['Return_Flag'] = np.where(train_df['Quantity_org']<0,1,0)\n",
    "\n",
    "train_df['Key'] = range(1, len(train_df)+1, 1)\n",
    "\n",
    "\n",
    "# Train DF for negative values only and for Description\n",
    "\n",
    "train_df_neg = train_df.copy()\n",
    "\n",
    "print(len(train_df))\n",
    "train_df = train_df[train_df['Quantity_org']>0]\n",
    "train_df = train_df[train_df['UnitPrice']>0]\n",
    "print(len(train_df))\n",
    "\n",
    "#train_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation\n",
    "#plt.figure(figsize=(15, 8))\n",
    "#sns.heatmap(train_df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target varibale distribution \n",
    "#train_df['UnitPrice'].plot(kind = 'density', title = 'Price Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Analysing distribution in Categorical varibales \\ncat_cols = ['StockCode', 'Description', 'InvoiceDate', 'CustomerID', 'Country']\\nfig, axes = plt.subplots(1,6, figsize=(24, 10))\\n\\nfor i, c in enumerate(cat_cols):\\n    _ = train_df[c].value_counts()[::-1].plot(kind = 'pie', ax=axes[i], title=c, autopct='%.0f', fontsize=18)\\n    _ = axes[i].set_ylabel('')\\n    \\n_ = plt.tight_layout()\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Analysing distribution in Categorical varibales \n",
    "cat_cols = ['StockCode', 'Description', 'InvoiceDate', 'CustomerID', 'Country']\n",
    "fig, axes = plt.subplots(1,6, figsize=(24, 10))\n",
    "\n",
    "for i, c in enumerate(cat_cols):\n",
    "    _ = train_df[c].value_counts()[::-1].plot(kind = 'pie', ax=axes[i], title=c, autopct='%.0f', fontsize=18)\n",
    "    _ = axes[i].set_ylabel('')\n",
    "    \n",
    "_ = plt.tight_layout()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking relation between quantity and Target\n",
    "\n",
    "#sns.jointplot(x=train_df['Quantity'], y=train_df['UnitPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"Test.csv\", parse_dates = [\"InvoiceDate\"])\n",
    "\n",
    "# Converting datetime to date objects.\n",
    "test_df['InvoiceDate_org'] = test_df['InvoiceDate']\n",
    "test_df['InvoiceDate'] = test_df['InvoiceDate'].apply(lambda x:x.date())\n",
    "test_df['CustomerID'] = test_df['CustomerID'].apply(lambda x:str(int(float(x))))\n",
    "test_df['Quantity_org'] = test_df['Quantity']\n",
    "\n",
    "test_df['Return_Flag'] = np.where(test_df['Quantity_org']<0,1,0)\n",
    "test_df['Quantity'] = test_df['Quantity'].apply(abs)\n",
    "#print(test_df.head())\n",
    "\n",
    "test_df['Key'] = range(1, len(test_df)+1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Test and Train and creating new Days since first sold var at Desc level\n",
    "\n",
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "train_df_neg['is_Train'] = 1\n",
    "\n",
    "df = pd.concat([train_df,test_df],ignore_index=True)\n",
    "df_neg = pd.concat([train_df_neg,test_df],ignore_index=True)\n",
    "\n",
    "df_first_date_desc = df.groupby('Description', as_index = False).agg({ 'InvoiceDate': 'min'}).\\\n",
    "                                    rename(columns = {'InvoiceDate': 'Min_Date_Desc'})\n",
    "\n",
    "df_neg_first_date_desc = df_neg.groupby('Description', as_index = False).agg({ 'InvoiceDate': 'min'}).\\\n",
    "                                    rename(columns = {'InvoiceDate': 'Min_Date_Desc'})\n",
    "\n",
    "df_first_date_stock = df.groupby(['Description', 'StockCode'], as_index = False).agg({ 'InvoiceDate': 'min'}).\\\n",
    "                                    rename(columns = {'InvoiceDate': 'Min_Date_Stock'})\n",
    "\n",
    "df = df.merge(df_first_date_desc, on = ['Description'], how = 'left')\n",
    "df = df.merge(df_first_date_stock, on = ['Description', 'StockCode'], how = 'left')\n",
    "df_neg = df_neg.merge(df_neg_first_date_desc, on = ['Description'], how = 'left')\n",
    "\n",
    "df['days_since_desc'] = df.apply(lambda x : (x['InvoiceDate'] - x['Min_Date_Desc']).days, axis =1)\n",
    "df['days_since_stock'] = df.apply(lambda x : (x['InvoiceDate'] - x['Min_Date_Stock']).days, axis =1)\n",
    "\n",
    "df_neg['days_since_desc'] = df_neg.apply(lambda x : (x['InvoiceDate'] - x['Min_Date_Desc']).days, axis =1)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking relation between days_since_desc and Target\n",
    "#train_data = df[df['is_train']==1]\n",
    "#sns.jointplot(x=train_data['days_since_desc'], y=train_data['UnitPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Calendar Date\n",
    "\n",
    "min_date = df['InvoiceDate'].min()\n",
    "df['days_since_calendar'] = df.apply(lambda x : (x['InvoiceDate'] - min_date).days, axis =1)\n",
    "\n",
    "#df['days_since_calendar'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Addding New Variable CountOfOrders, CountOfDistinctDate, NoOfUniquePricePoints, NoOfUniqueCustomers, NoOfCountries,\\nCountry35 Flag for each row\\nDesc level -> Price min,max,median,mode,mean,var\\nDesc level -> Quantity min,max,median,mode,mean,var\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278579\n",
      "122049\n",
      "400628\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Addding New Variable CountOfOrders, CountOfDistinctDate, NoOfUniquePricePoints, NoOfUniqueCustomers, NoOfCountries,\n",
    "Country35 Flag for each row\n",
    "Desc level -> Price min,max,median,mode,mean,var\n",
    "Desc level -> Quantity min,max,median,mode,mean,var\n",
    "\"\"\"\n",
    "\n",
    "df['NoOfOrders'] = df.groupby(['Description'])['InvoiceNo'].transform('count')\n",
    "df['CountOfDistinctDate'] = df.groupby(['Description'])['InvoiceDate'].transform(pd.Series.nunique)\n",
    "df['NoOfUniquePricePoints'] = df.groupby(['Description'])['UnitPrice'].transform(pd.Series.nunique)\n",
    "df['NoOfUniqueCustomers'] = df.groupby(['Description'])['CustomerID'].transform(pd.Series.nunique)\n",
    "df['NoOfCountries'] = df.groupby(['Description'])['Country'].transform(pd.Series.nunique)\n",
    "df['NoOfUniqueStocks'] = df.groupby(['Description'])['StockCode'].transform(pd.Series.nunique)\n",
    "df['Country35'] = np.where(df['Country']==35,1,0)\n",
    "df['DataAvailable'] = df.groupby(['Description'])['days_since_desc'].transform('max')\n",
    "\n",
    "df_neg['Country35'] = np.where(df_neg['Country']==35,1,0)\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Addding New Variable for\\nDesc level -> Price min,max,median,mode,mean,var\\nDesc level -> Quantity min,max,median,mode,mean,var\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Addding New Variable for\n",
    "Desc level -> Price min,max,median,mode,mean,var\n",
    "Desc level -> Quantity min,max,median,mode,mean,var\n",
    "\"\"\"\n",
    "\n",
    "df['Price_Mean'] = df.groupby(['Description'])['UnitPrice'].transform('mean')\n",
    "df['Price_Median'] = df.groupby(['Description'])['UnitPrice'].transform('median')\n",
    "df['Price_Max'] = df.groupby(['Description'])['UnitPrice'].transform('max')\n",
    "df['Price_Min'] = df.groupby(['Description'])['UnitPrice'].transform('min')\n",
    "df['Price_StDev'] = df.groupby(['Description'])['UnitPrice'].transform('std')\n",
    "\n",
    "df['Price_Mean_SC'] = df.groupby(['StockCode'])['UnitPrice'].transform('mean')\n",
    "df['Price_Median_SC'] = df.groupby(['StockCode'])['UnitPrice'].transform('median')\n",
    "df['Price_Max_SC'] = df.groupby(['StockCode'])['UnitPrice'].transform('max')\n",
    "df['Price_Min_SC'] = df.groupby(['StockCode'])['UnitPrice'].transform('min')\n",
    "df['Price_StDev_SC'] = df.groupby(['StockCode'])['UnitPrice'].transform('std')\n",
    "\n",
    "df['Price_Mean_Cust'] = df.groupby(['CustomerID'])['UnitPrice'].transform('mean')\n",
    "df['Price_Median_Cust'] = df.groupby(['CustomerID'])['UnitPrice'].transform('median')\n",
    "df['Price_Max_Cust'] = df.groupby(['CustomerID'])['UnitPrice'].transform('max')\n",
    "df['Price_Min_Cust'] = df.groupby(['CustomerID'])['UnitPrice'].transform('min')\n",
    "df['Price_StDev_Cust'] = df.groupby(['CustomerID'])['UnitPrice'].transform('std')\n",
    "\n",
    "df['Quantity_Mean_Cust'] = df.groupby(['CustomerID'])['Quantity'].transform('mean')\n",
    "df['Quantity_Median_Cust'] = df.groupby(['CustomerID'])['Quantity'].transform('median')\n",
    "df['Quantity_Max_Cust'] = df.groupby(['CustomerID'])['Quantity'].transform('max')\n",
    "df['Quantity_Min_Cust'] = df.groupby(['CustomerID'])['Quantity'].transform('min')\n",
    "df['Quantity_StDev_Cust'] = df.groupby(['CustomerID'])['Quantity'].transform('std')\n",
    "\n",
    "df['Price_Mean_Country'] = df.groupby(['Country'])['UnitPrice'].transform('mean')\n",
    "df['Price_Median_Country'] = df.groupby(['Country'])['UnitPrice'].transform('median')\n",
    "df['Price_Max_Country'] = df.groupby(['Country'])['UnitPrice'].transform('max')\n",
    "df['Price_Min_Country'] = df.groupby(['Country'])['UnitPrice'].transform('min')\n",
    "df['Price_StDev_Country'] = df.groupby(['Country'])['UnitPrice'].transform('std')\n",
    "\n",
    "df['Price_Mean_DescCoutry'] = df.groupby(['Country'])['UnitPrice'].transform('mean')\n",
    "df['Price_Median_DescCoutry'] = df.groupby(['Country'])['UnitPrice'].transform('median')\n",
    "df['Price_Max_DescCoutry'] = df.groupby(['Country'])['UnitPrice'].transform('max')\n",
    "df['Price_Min_DescCoutry'] = df.groupby(['Country'])['UnitPrice'].transform('min')\n",
    "df['Price_StDev_DescCoutry'] = df.groupby(['Country'])['UnitPrice'].transform('std')\n",
    "\n",
    "\n",
    "df['Quantity_Mean'] = df.groupby(['Description'])['Quantity'].transform('mean')\n",
    "df['Quantity_Median'] = df.groupby(['Description'])['Quantity'].transform('median')\n",
    "df['Quantity_Max'] = df.groupby(['Description'])['Quantity'].transform('max')\n",
    "df['Quantity_Min'] = df.groupby(['Description'])['Quantity'].transform('min')\n",
    "df['Quantity_StDev'] = df.groupby(['Description'])['Quantity'].transform('std')\n",
    "\n",
    "\n",
    "# Total Buy Of Customer in Train and Total Quantity \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Addding New Variable for\\nDesc level -> Price min,max,median,mode,mean,var\\nDesc level -> Quantity min,max,median,mode,mean,var\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Addding New Variable for\n",
    "Desc level -> Price min,max,median,mode,mean,var\n",
    "Desc level -> Quantity min,max,median,mode,mean,var\n",
    "\"\"\"\n",
    "\n",
    "df_neg['Price_Mean'] = df_neg.groupby(['Description'])['UnitPrice'].transform('mean')\n",
    "df_neg['Price_Median'] = df_neg.groupby(['Description'])['UnitPrice'].transform('median')\n",
    "df_neg['Price_Max'] = df_neg.groupby(['Description'])['UnitPrice'].transform('max')\n",
    "df_neg['Price_Min'] = df_neg.groupby(['Description'])['UnitPrice'].transform('min')\n",
    "df_neg['Price_StDev'] = df_neg.groupby(['Description'])['UnitPrice'].transform('std')\n",
    "\n",
    "df_neg['Price_Mean_SC'] = df_neg.groupby(['StockCode'])['UnitPrice'].transform('mean')\n",
    "df_neg['Price_Median_SC'] = df_neg.groupby(['StockCode'])['UnitPrice'].transform('median')\n",
    "df_neg['Price_Max_SC'] = df_neg.groupby(['StockCode'])['UnitPrice'].transform('max')\n",
    "df_neg['Price_Min_SC'] = df_neg.groupby(['StockCode'])['UnitPrice'].transform('min')\n",
    "df_neg['Price_StDev_SC'] = df_neg.groupby(['StockCode'])['UnitPrice'].transform('std')\n",
    "\n",
    "df_neg['Price_Mean_Cust'] = df_neg.groupby(['CustomerID'])['UnitPrice'].transform('mean')\n",
    "df_neg['Price_Median_Cust'] = df_neg.groupby(['CustomerID'])['UnitPrice'].transform('median')\n",
    "df_neg['Price_Max_Cust'] = df_neg.groupby(['CustomerID'])['UnitPrice'].transform('max')\n",
    "df_neg['Price_Min_Cust'] = df_neg.groupby(['CustomerID'])['UnitPrice'].transform('min')\n",
    "df_neg['Price_StDev_Cust'] = df_neg.groupby(['CustomerID'])['UnitPrice'].transform('std')\n",
    "\n",
    "df_neg['Quantity_Mean_Cust'] = df_neg.groupby(['CustomerID'])['Quantity'].transform('mean')\n",
    "df_neg['Quantity_Median_Cust'] = df_neg.groupby(['CustomerID'])['Quantity'].transform('median')\n",
    "df_neg['Quantity_Max_Cust'] = df_neg.groupby(['CustomerID'])['Quantity'].transform('max')\n",
    "df_neg['Quantity_Min_Cust'] = df_neg.groupby(['CustomerID'])['Quantity'].transform('min')\n",
    "df_neg['Quantity_StDev_Cust'] = df_neg.groupby(['CustomerID'])['Quantity'].transform('std')\n",
    "\n",
    "df_neg['Price_Mean_Country'] = df_neg.groupby(['Country'])['UnitPrice'].transform('mean')\n",
    "df_neg['Price_Median_Country'] = df_neg.groupby(['Country'])['UnitPrice'].transform('median')\n",
    "df_neg['Price_Max_Country'] = df_neg.groupby(['Country'])['UnitPrice'].transform('max')\n",
    "df_neg['Price_Min_Country'] = df_neg.groupby(['Country'])['UnitPrice'].transform('min')\n",
    "df_neg['Price_StDev_Country'] = df_neg.groupby(['Country'])['UnitPrice'].transform('std')\n",
    "\n",
    "df_neg['Price_Mean_DescCoutry'] = df_neg.groupby(['Country'])['UnitPrice'].transform('mean')\n",
    "df_neg['Price_Median_DescCoutry'] = df_neg.groupby(['Country'])['UnitPrice'].transform('median')\n",
    "df_neg['Price_Max_DescCoutry'] = df_neg.groupby(['Country'])['UnitPrice'].transform('max')\n",
    "df_neg['Price_Min_DescCoutry'] = df_neg.groupby(['Country'])['UnitPrice'].transform('min')\n",
    "df_neg['Price_StDev_DescCoutry'] = df_neg.groupby(['Country'])['UnitPrice'].transform('std')\n",
    "\n",
    "\n",
    "df_neg['Quantity_Mean'] = df_neg.groupby(['Description'])['Quantity'].transform('mean')\n",
    "df_neg['Quantity_Median'] = df_neg.groupby(['Description'])['Quantity'].transform('median')\n",
    "df_neg['Quantity_Max'] = df_neg.groupby(['Description'])['Quantity'].transform('max')\n",
    "df_neg['Quantity_Min'] = df_neg.groupby(['Description'])['Quantity'].transform('min')\n",
    "df_neg['Quantity_StDev'] = df_neg.groupby(['Description'])['Quantity'].transform('std')\n",
    "\n",
    "\n",
    "# Total Buy Of Customer in Train and Total Quantity \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400628\n",
      "406829\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Mode Values  for Price and Quantity\n",
    "\n",
    "def getMode1(x):\n",
    "    return x.value_counts().index[0]\n",
    "\n",
    "def getMode2(x):\n",
    "    if len(x.value_counts())>1:\n",
    "        return x.value_counts().index[1]\n",
    "    else:\n",
    "        return x.value_counts().index[0]\n",
    "\n",
    "train_agg = train_df.groupby(['Description'], as_index = False).agg(\\\n",
    "                                            Price_Mode1 = ('UnitPrice', getMode1),\\\n",
    "                                            Price_Mode2 = ('UnitPrice', getMode2),\\\n",
    "                                            Quantity_Mode = ('Quantity', getMode1))\n",
    "\n",
    "df = df.merge(train_agg, on = ['Description'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking relation between Description and Price_Mean\n",
    "#train_data = df[df['is_train']==1]\n",
    "#sns.jointplot(x=train_data['Description'], y=train_data['Price_Mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Flag for bad Customers\n",
    "\n",
    "bad_customer_list = ['14096','17450','14298','14088']\n",
    "\n",
    "df['bad_customer_flag'] = np.where(df['CustomerID'].isin(bad_customer_list),1,0)\n",
    "df_neg['bad_customer_flag'] = np.where(df_neg['CustomerID'].isin(bad_customer_list),1,0)\n",
    "\n",
    "#df.to_csv('Combined_Df_with_vars.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Personal Encoder on Min Max value\n",
    "\n",
    "#desc_train1 = train_df['Price_Max', 'Price_Min', 'Price_Mode'].drop_duplicates()\n",
    "\n",
    "df['Price_Mode_Sum'] = df['Price_Mode1'] + df['Price_Mode2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.preprocessing import OneHotEncoder\\n# creating one hot encoder object \\nonehotencoder = OneHotEncoder()\\n\\nencoder = OneHotEncoder(sparse=False)\\ntrain_X_encoded = pd.DataFrame (encoder.fit_transform(df[['Country']]))\\ntrain_X_encoded.columns = encoder.get_feature_names(['Country'])\\n\\ndf = pd.concat([df, train_X_encoded], axis=1)\\n#droping the country column \\n#df= df.drop(['Country'], axis=1) \\n#printing to verify \\ndf.head()\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_X_encoded = pd.DataFrame (encoder.fit_transform(df[['Country']]))\n",
    "train_X_encoded.columns = encoder.get_feature_names(['Country'])\n",
    "\n",
    "df = pd.concat([df, train_X_encoded], axis=1)\n",
    "#droping the country column \n",
    "#df= df.drop(['Country'], axis=1) \n",
    "#printing to verify \n",
    "df.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[encoder.get_feature_names(['Country'])].sum()\n",
    "\n",
    "#encoder.get_feature_names(['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284780\n",
      "278579\n",
      "122049\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = df[df['is_train']==1]\n",
    "test_df = df[df['is_train']==0]\n",
    "train_df_neg = df_neg[df_neg['is_Train']==1]\n",
    "\n",
    "print(len(train_df_neg))\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "\n",
    "#desc_train2 = train_df['Price_Max', 'Price_Min', 'Price_Mode'].drop_duplicates()\n",
    "\n",
    "#prine(len(dist_train))\n",
    "\n",
    "#df.to_csv('df_with_all_vals.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = df[df['is_train']==1]\n",
    "#sns.jointplot(x=train_df['Price_Mode_Sum'], y=train_df['UnitPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom category_encoders.ordinal import OrdinalEncoder\\nfrom category_encoders.woe import WOEEncoder\\nfrom category_encoders.target_encoder import TargetEncoder\\nfrom category_encoders.sum_coding import SumEncoder\\nfrom category_encoders.m_estimate import MEstimateEncoder\\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\\nfrom category_encoders.helmert import HelmertEncoder\\nfrom category_encoders.cat_boost import CatBoostEncoder\\nfrom category_encoders.james_stein import JamesSteinEncoder\\nfrom category_encoders.one_hot import OneHotEncoder\\n\\n\\nfeature_list = ['Description']\\n#target = 'UnitPrice'\\ntarget = 'UnitPrice'\\n\\n\\nCBE_encoder = CatBoostEncoder()\\ntrain_cbe = CBE_encoder.fit_transform(train_df[feature_list], train_df[target])\\ntest_cbe = CBE_encoder.transform(test_df[feature_list])\\n\\ntrain_df['NewDescription'] = train_cbe\\n\\nprint(train_df['NewDescription'].nunique())\\nprint(train_df['Description'].nunique())\\n\\ntrain_df.to_csv('Encoded_Train.csv', index = False)\\n\\n\\n\\nT_encoder = MEstimateEncoder()\\ntrain_cbe = T_encoder.fit_transform(train_df[feature_list], train_df[target])\\ntest_cbe = T_encoder.transform(test_df[feature_list])\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nMEE_encoder = MEstimateEncoder()\\ntrain_mee = MEE_encoder.fit_transform(train[feature_list], target)\\ntest_mee = MEE_encoder.transform(test[feature_list])\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Category Encoding based on target data\n",
    "\n",
    "\"\"\"\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "\n",
    "feature_list = ['Description']\n",
    "#target = 'UnitPrice'\n",
    "target = 'UnitPrice'\n",
    "\n",
    "\n",
    "CBE_encoder = CatBoostEncoder()\n",
    "train_cbe = CBE_encoder.fit_transform(train_df[feature_list], train_df[target])\n",
    "test_cbe = CBE_encoder.transform(test_df[feature_list])\n",
    "\n",
    "train_df['NewDescription'] = train_cbe\n",
    "\n",
    "print(train_df['NewDescription'].nunique())\n",
    "print(train_df['Description'].nunique())\n",
    "\n",
    "train_df.to_csv('Encoded_Train.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "T_encoder = MEstimateEncoder()\n",
    "train_cbe = T_encoder.fit_transform(train_df[feature_list], train_df[target])\n",
    "test_cbe = T_encoder.transform(test_df[feature_list])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "MEE_encoder = MEstimateEncoder()\n",
    "train_mee = MEE_encoder.fit_transform(train[feature_list], target)\n",
    "test_mee = MEE_encoder.transform(test[feature_list])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,KFold,StratifiedKFold\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_boosting(clf,k, fit_params, train, test, features):\n",
    "    N_SPLITS = k\n",
    "    oofs = np.zeros(len(train))\n",
    "    preds = np.zeros((len(test)))\n",
    "\n",
    "    folds = KFold(n_splits = N_SPLITS,random_state=42)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET_COL])):\n",
    "        print(f'\\n------------- Fold {fold_ + 1} -------------')\n",
    "        target = train[TARGET_COL]\n",
    "        X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n",
    "\n",
    "        X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n",
    "\n",
    "        X_test = test[features]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = preprocessing.MinMaxScaler()\n",
    "        #scaler = preprocessing.RobustScaler()\n",
    "        _ = scaler.fit(X_trn)\n",
    "\n",
    "        X_trn = scaler.transform(X_trn)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n",
    "        preds_val = clf.predict(X_val)\n",
    "        preds_test = clf.predict(X_test)\n",
    "        #preds_test1 = (np.exp(preds_test)-1)\n",
    "        \n",
    "        fold_score =  np.sqrt(mean_squared_error(y_val,preds_val)) \n",
    "        print(f'\\n RMSE score for validation set is {fold_score}')\n",
    "\n",
    "        oofs[val_idx] = preds_val\n",
    "        preds += preds_test / N_SPLITS\n",
    "\n",
    "\n",
    "    oofs_score = np.sqrt(mean_squared_error(target, oofs))\n",
    "    print(f'\\n\\n rmse score for oofs is {oofs_score}')\n",
    "    return oofs, preds, oofs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country', 'InvoiceDate_org', 'Quantity_org', 'Return_Flag', 'Key', 'is_train', 'Min_Date_Desc', 'Min_Date_Stock', 'days_since_desc', 'days_since_stock', 'days_since_calendar', 'NoOfOrders', 'CountOfDistinctDate', 'NoOfUniquePricePoints', 'NoOfUniqueCustomers', 'NoOfCountries', 'NoOfUniqueStocks', 'Country35', 'DataAvailable', 'Price_Mean', 'Price_Median', 'Price_Max', 'Price_Min', 'Price_StDev', 'Price_Mean_SC', 'Price_Median_SC', 'Price_Max_SC', 'Price_Min_SC', 'Price_StDev_SC', 'Price_Mean_Cust', 'Price_Median_Cust', 'Price_Max_Cust', 'Price_Min_Cust', 'Price_StDev_Cust', 'Quantity_Mean_Cust', 'Quantity_Median_Cust', 'Quantity_Max_Cust', 'Quantity_Min_Cust', 'Quantity_StDev_Cust', 'Price_Mean_Country', 'Price_Median_Country', 'Price_Max_Country', 'Price_Min_Country', 'Price_StDev_Country', 'Price_Mean_DescCoutry', 'Price_Median_DescCoutry', 'Price_Max_DescCoutry', 'Price_Min_DescCoutry', 'Price_StDev_DescCoutry', 'Quantity_Mean', 'Quantity_Median', 'Quantity_Max', 'Quantity_Min', 'Quantity_StDev', 'Price_Mode1', 'Price_Mode2', 'Quantity_Mode', 'bad_customer_flag', 'Price_Mode_Sum'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#All cols in current data\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Columns to take in the model\n",
    "model_col = [ \\\n",
    "            'Price_Mean','Price_Median' ,'Price_Max', 'Price_Min', 'Price_StDev','Quantity',\\\n",
    "             'days_since_desc','Price_Mode1','Country35',\\\n",
    "             #'days_since_calendar'\n",
    "             #'Price_Mean_Country', 'Price_Median_Country', 'Price_Max_Country', 'Price_Min_Country', 'Price_StDev_Country',\\\n",
    "             #'Quantity_Median_Cust',\\\n",
    "             'Quantity_Mean','Quantity_Median', 'Quantity_Max', 'Quantity_Min', 'Quantity_StDev', 'Quantity_Mode',\\\n",
    "             'Price_Mean_Cust', 'Price_Median_Cust', 'Price_Max_Cust', 'Price_Min_Cust', 'Price_StDev_Cust',\\\n",
    "             'Price_Mean_DescCoutry', 'Price_Median_DescCoutry', 'Price_Max_DescCoutry', 'Price_Min_DescCoutry',\\\n",
    "             'Price_StDev_DescCoutry'\n",
    "            #'bad_customer_flag'\\\n",
    "            ]\n",
    "\n",
    "\n",
    "cat_col = ['Description', 'StockCode']\n",
    "tar_col = ['UnitPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df['Key'] = range(1, len(test_df)+1, 1)\n",
    "#train_df['Key'] = range(1, len(train_df)+1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278579\n",
      "122049\n",
      "273208\n",
      "119734\n"
     ]
    }
   ],
   "source": [
    "desc_list = [2624, 2140]\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "\n",
    "#test_df.to_csv('Testiiing.csv', index = False)\n",
    "\n",
    "#\n",
    "#train_df1 = train_df[((train_df['bad_customer_flag'] == 0) & ~(train_df['Description'].isin(desc_list)))]\n",
    "#test_df1 = test_df[((test_df['bad_customer_flag'] == 0) & ~(test_df['Description'].isin(desc_list)))]\n",
    "\n",
    "train_df1 = train_df[(train_df['bad_customer_flag'] == 0) ]\n",
    "test_df1 = test_df[(test_df['bad_customer_flag'] == 0) ]\n",
    "\n",
    "\n",
    "#\n",
    "train_df2 = train_df[train_df['bad_customer_flag'] == 1]\n",
    "test_df2 = test_df[test_df['bad_customer_flag'] == 1]\n",
    "\n",
    "\n",
    "print(len(train_df1))\n",
    "print(len(test_df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Fold 1 -------------\n",
      "\n",
      " RMSE score for validation set is 5.974283329530361\n",
      "\n",
      "------------- Fold 2 -------------\n",
      "\n",
      " RMSE score for validation set is 3.6491043916425343\n",
      "\n",
      "------------- Fold 3 -------------\n",
      "\n",
      " RMSE score for validation set is 14.072228046600305\n",
      "\n",
      "------------- Fold 4 -------------\n",
      "\n",
      " RMSE score for validation set is 3.3724309790901574\n",
      "\n",
      "------------- Fold 5 -------------\n",
      "\n",
      " RMSE score for validation set is 4.24791324688471\n",
      "\n",
      "------------- Fold 6 -------------\n",
      "\n",
      " RMSE score for validation set is 1.6171446159015563\n",
      "\n",
      "------------- Fold 7 -------------\n",
      "\n",
      " RMSE score for validation set is 1.8946169946689098\n",
      "\n",
      "------------- Fold 8 -------------\n",
      "\n",
      " RMSE score for validation set is 8.848543313262526\n",
      "\n",
      "------------- Fold 9 -------------\n",
      "\n",
      " RMSE score for validation set is 7.129191243318177\n",
      "\n",
      "------------- Fold 10 -------------\n",
      "\n",
      " RMSE score for validation set is 5.343175955791143\n",
      "\n",
      "\n",
      " rmse score for oofs is 6.63573920707743\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xgb = XGBRegressor(n_estimators = 10000,\n",
    "#                        learning_rate = 0.05,\n",
    "#                       colsample_bytree = 0.76,\n",
    "                        )\n",
    "fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "TARGET_COL= 'UnitPrice'\n",
    "features = model_col\n",
    "xgb_oofs, xgb_preds,score_xgb = run_gradient_boosting(xgb,10, fit_params, train_df1, test_df1, features)\n",
    "\n",
    "train_df1['y_xgb'] = xgb_oofs\n",
    "test_df1['y_xgb'] = xgb_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Fold 1 -------------\n",
      "\n",
      " RMSE score for validation set is 5.924738418025408\n",
      "\n",
      "------------- Fold 2 -------------\n",
      "\n",
      " RMSE score for validation set is 3.995389782213259\n",
      "\n",
      "------------- Fold 3 -------------\n",
      "\n",
      " RMSE score for validation set is 13.164928029330326\n",
      "\n",
      "------------- Fold 4 -------------\n",
      "\n",
      " RMSE score for validation set is 4.789895208065252\n",
      "\n",
      "------------- Fold 5 -------------\n",
      "\n",
      " RMSE score for validation set is 3.1068305506701273\n",
      "\n",
      "------------- Fold 6 -------------\n",
      "\n",
      " RMSE score for validation set is 1.9873594159411676\n",
      "\n",
      "------------- Fold 7 -------------\n",
      "\n",
      " RMSE score for validation set is 2.848835179963958\n",
      "\n",
      "------------- Fold 8 -------------\n",
      "\n",
      " RMSE score for validation set is 6.571711174288212\n",
      "\n",
      "------------- Fold 9 -------------\n",
      "\n",
      " RMSE score for validation set is 4.028852705005997\n",
      "\n",
      "------------- Fold 10 -------------\n",
      "\n",
      " RMSE score for validation set is 3.5872038236722346\n",
      "\n",
      "\n",
      " rmse score for oofs is 5.842349296904673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lgb = LGBMRegressor(n_estimators = 10000, importance_type='gain',\n",
    "#                          learning_rate = 0.05,\n",
    "#                          colsample_bytree = 0.76,\n",
    "                        )\n",
    "fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "TARGET_COL= 'UnitPrice'\n",
    "features = model_col\n",
    "lgb_oofs, lgb_preds, score_lgb = run_gradient_boosting(lgb,10, fit_params, train_df1, test_df1, features)\n",
    "\n",
    "train_df1['y_lgb'] = lgb_oofs\n",
    "test_df1['y_lgb'] = lgb_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Fold 1 -------------\n",
      "\n",
      " RMSE score for validation set is 3.256334865114697\n",
      "\n",
      "------------- Fold 2 -------------\n",
      "\n",
      " RMSE score for validation set is 1.2050423804510115\n",
      "\n",
      "------------- Fold 3 -------------\n",
      "\n",
      " RMSE score for validation set is 12.294222985189249\n",
      "\n",
      "------------- Fold 4 -------------\n",
      "\n",
      " RMSE score for validation set is 2.7702326709048393\n",
      "\n",
      "------------- Fold 5 -------------\n",
      "\n",
      " RMSE score for validation set is 1.480320712035936\n",
      "\n",
      "------------- Fold 6 -------------\n",
      "\n",
      " RMSE score for validation set is 1.5807461327563748\n",
      "\n",
      "------------- Fold 7 -------------\n",
      "\n",
      " RMSE score for validation set is 1.8816086539204855\n",
      "\n",
      "------------- Fold 8 -------------\n",
      "\n",
      " RMSE score for validation set is 5.981248238435534\n",
      "\n",
      "------------- Fold 9 -------------\n",
      "\n",
      " RMSE score for validation set is 4.336102596070534\n",
      "\n",
      "------------- Fold 10 -------------\n",
      "\n",
      " RMSE score for validation set is 2.8976517218306617\n",
      "\n",
      "\n",
      " rmse score for oofs is 4.9201909411474825\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostRegressor(iterations=10000,\n",
    "                       learning_rate = 0.1,\n",
    "#                         colsample_bytree = 0.76,\n",
    "                        )\n",
    "\n",
    "fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "TARGET_COL= 'UnitPrice'\n",
    "\n",
    "#train_df1['CustomerIDA'] = train_df1['CustomerID']+'A'\n",
    "#test_df1['CustomerIDA'] = test_df1['CustomerID']+'A'\n",
    "\n",
    "features = model_col\n",
    "\n",
    "cb_oofs, cb_preds, scores_cb = run_gradient_boosting(cb,10, fit_params, train_df1, test_df1, features)\n",
    "\n",
    "train_df1['y_cb'] = cb_oofs\n",
    "test_df1['y_cb'] = cb_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB 6.63573920707743\n",
      "LGB 5.842349296904673\n",
      "CB 4.9201909411474825\n",
      "XGB 2.8537366457337447\n",
      "LGB 1.990223335117116\n",
      "CB 0.8572806379070284\n"
     ]
    }
   ],
   "source": [
    "# Here You can even extract your training data and analyse on which outcomes you are getting most error\n",
    "# and take further step accordingly\n",
    "\n",
    "print('XGB', np.sqrt(mean_squared_error(train_df1['UnitPrice'], train_df1['y_xgb'])))\n",
    "print('LGB', np.sqrt(mean_squared_error(train_df1['UnitPrice'], train_df1['y_lgb'])))\n",
    "print('CB', np.sqrt(mean_squared_error(train_df1['UnitPrice'], train_df1['y_cb'])))\n",
    "\n",
    "desc_list = [2624,2140]\n",
    "#desc_list = [2140]\n",
    "\n",
    "train_ch = train_df1[~(train_df1['Description'].isin( desc_list))]\n",
    "\n",
    "print('XGB', np.sqrt(mean_squared_error(train_ch['UnitPrice'], train_ch['y_xgb'])))\n",
    "print('LGB', np.sqrt(mean_squared_error(train_ch['UnitPrice'], train_ch['y_lgb'])))\n",
    "print('CB', np.sqrt(mean_squared_error(train_ch['UnitPrice'], train_ch['y_cb'])))\n",
    "\n",
    "#vis = train[model_col]\n",
    "#train_df1.to_csv('train_out.csv',index=False)\n",
    "\n",
    "\n",
    "#Initial\n",
    "#XGB 6.75708061980588\n",
    "#LGB 6.8533119642410485\n",
    "#CB 6.192579534217409\n",
    "#XGB 1.374504379844585\n",
    "#LGB 2.214419022731006\n",
    "#CB 1.6746977994468253\n",
    "\n",
    "\n",
    "#With cust variables only\n",
    "#XGB 6.62136934556261\n",
    "#LGB 6.0595596619508765\n",
    "#CB 5.090020090736837\n",
    "#XGB 2.9104907925809838\n",
    "#LGB 1.9568225605270688\n",
    "#CB 0.8598545791541393\n",
    "\n",
    "#With countrydesc and without cust variables\n",
    "#XGB 7.5361980307975465\n",
    "#LGB 6.414495187642012\n",
    "#CB 6.62048569699907\n",
    "#XGB 3.36982995579794\n",
    "#LGB 2.170495846545204\n",
    "#CB 2.0324233072277753\n",
    "\n",
    "#with countrydesc and cust variables\n",
    "#All\n",
    "#XGB 6.63573920707743\n",
    "#LGB 5.842349296904673\n",
    "#CB 4.9201909411474825\n",
    "#Remove Descriptions\n",
    "#XGB 2.8537366457337447\n",
    "#LGB 1.990223335117116\n",
    "#CB 0.8572806379070284\n",
    "\n",
    "# With CountryPrice Var\n",
    "#XGB 6.667124219826937\n",
    "#LGB 5.842349296904673\n",
    "#CB 5.14830477284481\n",
    "#XGB 2.923244780114524\n",
    "#LGB 1.990223335117116\n",
    "#CB 0.9715364217698965\n",
    "\n",
    "#With StockCode\n",
    "#XGB 6.585061062531734\n",
    "#LGB 5.904042617902602\n",
    "#CB 5.271007264281459\n",
    "#XGB 2.810480890310904\n",
    "#LGB 2.007563133657509\n",
    "#CB 0.9835345193264253\n",
    "\n",
    "#With StockCode Mean only\n",
    "#XGB 6.5814832385682545\n",
    "#LGB 5.90072246520115\n",
    "#CB 5.1718583089414185\n",
    "#XGB 2.807175281711653\n",
    "#LGB 2.0142463106278146\n",
    "#CB 1.0822050726769732\n",
    "\n",
    "# With CustomerQuantity\n",
    "#XGB 6.413276923221273\n",
    "#LGB 5.656222028715484\n",
    "#CB 5.220617493020439\n",
    "#XGB 2.8378301012186107\n",
    "#LGB 1.980336931615718\n",
    "#CB 0.8874526123657804\n",
    "\n",
    "# With Customer Quantity Mode\n",
    "#XGB 6.67639498035905\n",
    "#LGB 5.634793975234537\n",
    "#CB 5.001626564245285\n",
    "#XGB 2.8310387757103124\n",
    "#LGB 1.9829330418853484\n",
    "#CB 0.9743152626497211\n",
    "\n",
    "\n",
    "# Lets go with CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027236272179873"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1['y_merge'] = 0.1*train_df1['y_xgb'] + 0.1*train_df1['y_lgb']+ 0.8*train_df1['y_cb']\n",
    "test_df1['y_merge'] = 0.1*test_df1['y_xgb'] + 0.1*test_df1['y_lgb']+ 0.8*test_df1['y_cb']\n",
    "np.sqrt(mean_squared_error(train_df1['UnitPrice'], train_df1['y_merge']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------XGBooster-----------------\n",
      "\n",
      "------------- Fold 1 -------------\n",
      "\n",
      " RMSE score for validation set is 19.921509829761504\n",
      "\n",
      "------------- Fold 2 -------------\n",
      "\n",
      " RMSE score for validation set is 8.700223382011949\n",
      "\n",
      "------------- Fold 3 -------------\n",
      "\n",
      " RMSE score for validation set is 4.681825164446201\n",
      "\n",
      "------------- Fold 4 -------------\n",
      "\n",
      " RMSE score for validation set is 39.41147728890589\n",
      "\n",
      "------------- Fold 5 -------------\n",
      "\n",
      " RMSE score for validation set is 7.804104021127562\n",
      "\n",
      "------------- Fold 6 -------------\n",
      "\n",
      " RMSE score for validation set is 7.059205030821113\n",
      "\n",
      "------------- Fold 7 -------------\n",
      "\n",
      " RMSE score for validation set is 23.666303599342445\n",
      "\n",
      "------------- Fold 8 -------------\n",
      "\n",
      " RMSE score for validation set is 18.184841517548758\n",
      "\n",
      "------------- Fold 9 -------------\n",
      "\n",
      " RMSE score for validation set is 881.2707625360179\n",
      "\n",
      "------------- Fold 10 -------------\n",
      "\n",
      " RMSE score for validation set is 57.826873964956796\n",
      "\n",
      "\n",
      " rmse score for oofs is 279.82697645606765\n",
      "---------------CatBooster-----------------\n",
      "\n",
      "------------- Fold 1 -------------\n",
      "\n",
      " RMSE score for validation set is 20.394510124703164\n",
      "\n",
      "------------- Fold 2 -------------\n",
      "\n",
      " RMSE score for validation set is 8.606078522232915\n",
      "\n",
      "------------- Fold 3 -------------\n",
      "\n",
      " RMSE score for validation set is 7.1989901437399695\n",
      "\n",
      "------------- Fold 4 -------------\n",
      "\n",
      " RMSE score for validation set is 51.80726489810153\n",
      "\n",
      "------------- Fold 5 -------------\n",
      "\n",
      " RMSE score for validation set is 9.573473273523025\n",
      "\n",
      "------------- Fold 6 -------------\n",
      "\n",
      " RMSE score for validation set is 6.694461865066172\n",
      "\n",
      "------------- Fold 7 -------------\n",
      "\n",
      " RMSE score for validation set is 22.371714886731784\n",
      "\n",
      "------------- Fold 8 -------------\n",
      "\n",
      " RMSE score for validation set is 26.6480155701765\n",
      "\n",
      "------------- Fold 9 -------------\n",
      "\n",
      " RMSE score for validation set is 878.4902736911955\n",
      "\n",
      "------------- Fold 10 -------------\n",
      "\n",
      " RMSE score for validation set is 54.577671824664606\n",
      "\n",
      "\n",
      " rmse score for oofs is 279.1588758375569\n"
     ]
    }
   ],
   "source": [
    "# Create Model Only for Bad Descriptions\n",
    "\n",
    "\n",
    "desc_list = [2624]\n",
    "#desc_list = [2140]\n",
    "\n",
    "train_df_desc_2624 = train_df_neg[train_df_neg['Description'].isin(desc_list)]\n",
    "test_df_desc_2624 = test_df[test_df['Description'].isin(desc_list)]\n",
    "\n",
    "# Selecting Columns to take in the model\n",
    "model_col = [ \\\n",
    "            'Price_Mean','Price_Median' ,'Price_Max', 'Price_Min', 'Price_StDev','Quantity',\\\n",
    "             'days_since_desc','Country35','Return_Flag',\\\n",
    "             #'days_since_calendar'\n",
    "             #'Price_Mean_Country', 'Price_Median_Country', 'Price_Max_Country', 'Price_Min_Country', 'Price_StDev_Country',\\\n",
    "             #'Quantity_Median_Cust',\\\n",
    "             'Quantity_Mean','Quantity_Median', 'Quantity_Max', 'Quantity_Min', 'Quantity_StDev',\\\n",
    "             'Price_Mean_Cust', 'Price_Median_Cust', 'Price_Max_Cust', 'Price_Min_Cust', 'Price_StDev_Cust',\\\n",
    "             'Price_Mean_DescCoutry', 'Price_Median_DescCoutry', 'Price_Max_DescCoutry', 'Price_Min_DescCoutry',\\\n",
    "             'Price_StDev_DescCoutry'\n",
    "            #'bad_customer_flag'\\\n",
    "            ]\n",
    "\n",
    "cat_col = ['Description', 'StockCode']\n",
    "tar_col = ['UnitPrice']\n",
    "\n",
    "xgb = XGBRegressor(n_estimators = 1000,\n",
    "#                        learning_rate = 0.05,\n",
    "#                       colsample_bytree = 0.76,\n",
    "                        )\n",
    "fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "TARGET_COL= 'UnitPrice'\n",
    "features = model_col\n",
    "\n",
    "print('---------------XGBooster-----------------')\n",
    "xgb_oofs, xgb_preds,score_xgb = run_gradient_boosting(xgb,10, fit_params, train_df_desc_2624, test_df_desc_2624, features)\n",
    "\n",
    "cb = CatBoostRegressor(iterations=10000,\n",
    "                       learning_rate = 0.1,\n",
    "#                         colsample_bytree = 0.76,\n",
    "                        )\n",
    "\n",
    "print('---------------CatBooster-----------------')\n",
    "cb_oofs, cb_preds, scores_cb = run_gradient_boosting(cb,10, fit_params, train_df_desc_2624, test_df_desc_2624, features)\n",
    "\n",
    "train_df_desc_2624['y_xgb'] = xgb_oofs\n",
    "test_df_desc_2624['y_xgb'] = xgb_preds\n",
    "\n",
    "train_df_desc_2624['y_cb'] = cb_oofs\n",
    "test_df_desc_2624['y_cb'] = cb_preds\n",
    "\n",
    "\n",
    "test_df_desc_2624.to_csv('test_bad_desc.csv', index = False)\n",
    "train_df_desc_2624.to_csv('train_bad_desc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df1.columns\n",
    "#train_df_desc_2624.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB 279.82697645606765\n",
      "CB 279.1588758375569\n",
      "XGB 6.63573920707743\n",
      "CB 4.9201909411474825\n"
     ]
    }
   ],
   "source": [
    "# Combining with old values and Check the error Afterwards\n",
    "\n",
    "#train_er = train_df1[~(train_df1['Description'].isin( desc_list))]\n",
    "\n",
    "print('XGB', np.sqrt(mean_squared_error(train_df_desc_2624['UnitPrice'], train_df_desc_2624['y_xgb'])))\n",
    "print('CB', np.sqrt(mean_squared_error(train_df_desc_2624['UnitPrice'], train_df_desc_2624['y_cb'])))\n",
    "\n",
    "\n",
    "test_df_desc_2624_fm = test_df_desc_2624[['Key','y_xgb', 'y_cb']].rename(columns = {'y_xgb':'y_xgb_baddesc',\\\n",
    "                                                                                    'y_cb':'y_cb_baddesc'})\n",
    "\n",
    "train_df1_merged = train_df1.merge(test_df_desc_2624_fm, on = 'Key', how = 'left')\n",
    "\n",
    "train_df1_merged['y_xgb_merged'] = np.where(train_df1_merged['y_xgb_baddesc'].isna(),train_df1_merged['y_xgb']\\\n",
    "                                            ,train_df1_merged['y_xgb_baddesc'])\n",
    "train_df1_merged['y_cb_merged'] = np.where(train_df1_merged['y_cb_baddesc'].isna(),train_df1_merged['y_cb']\\\n",
    "                                            ,train_df1_merged['y_cb_baddesc'])\n",
    "\n",
    "\n",
    "print('XGB', np.sqrt(mean_squared_error(train_df1_merged['UnitPrice'], train_df1_merged['y_xgb'])))\n",
    "print('CB', np.sqrt(mean_squared_error(train_df1_merged['UnitPrice'], train_df1_merged['y_cb'])))\n",
    "\n",
    "#vis = train[model_col]\n",
    "#train_df1.to_csv('train_out.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad Customer data\n",
    "\n",
    "score_df_badcust_xgb = pd.DataFrame()\n",
    "train_final_badcust_xgb = pd.DataFrame()\n",
    "test_final_badcust_xgb = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for desc,train_d1 in train_df[train_df['bad_customer_flag'] == 1].groupby(['CustomerID','Description']):\n",
    "    #print('----------------------',desc,'----------------------')\n",
    "    xgb = XGBRegressor(n_estimators = 100,\n",
    "    #                        learning_rate = 0.05,\n",
    "    #                       colsample_bytree = 0.76,\n",
    "                            )\n",
    "    \n",
    "    fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "    TARGET_COL= 'UnitPrice'\n",
    "    features = model_col\n",
    "    test_d1 = test_df[(test_df['Description'] == desc[1]) & (test_df['CustomerID'] == desc[0])]\n",
    "    split_val = min(len(train_d1),10)\n",
    "    \n",
    "    if len(test_d1)>0 and len(train_d1)>1:\n",
    "        xgb_oofs, xgb_preds, score = run_gradient_boosting(xgb, split_val, fit_params, train_d1, test_d1, features)\n",
    "        \n",
    "    elif len(train_d1) == 1:\n",
    "        un_val =  list(train_d1['UnitPrice'].unique())[0]\n",
    "        xgb_oofs, xgb_preds, score = np.zeros(len(train_d1)), np.repeat(un_val,len(test_d1)), 0\n",
    "        \n",
    "    else:\n",
    "        xgb_oofs, xgb_preds, score = np.zeros(len(train_d1)), np.zeros(len(test_d1)), 0\n",
    "        \n",
    "    temp_df = pd.DataFrame({'Description':[desc[1]], 'CustomerID':[desc[0]] ,'Price_Mode':[desc], 'xgb_score':[score],\\\n",
    "                            'Length_of_train':[len(train_d1)],\\\n",
    "                            'Length_of_test':[len(test_d1)],'unique_Desc':[train_d1['Description'].nunique()]})\n",
    "    \n",
    "    score_df_badcust_xgb = score_df_badcust_xgb.append(temp_df)\n",
    "    \n",
    "    # Adding in train data\n",
    "    # Adding in test data\n",
    "    train_d1['y_xgb'] = xgb_oofs\n",
    "    test_d1['y_xgb'] = xgb_preds\n",
    "    train_final_badcust_xgb = train_final_badcust_xgb.append(train_d1)\n",
    "    test_final_badcust_xgb = test_final_badcust_xgb.append(test_d1)\n",
    "    i += 1\n",
    "    #if i>=10:\n",
    "    #    break\n",
    "        \n",
    "\n",
    "score_df_badcust_xgb.to_csv('xgb_10_fold_score_bad_cust.csv', index = False)\n",
    "train_final_badcust_xgb.to_csv('xgb_10_fold_train_bad_cust.csv', index = False)\n",
    "test_final_badcust_xgb.to_csv('xgb_10_fold_test_bad_cust.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Price Point and No Of Samples >=10 then override with Mode Price for all test values\n",
    "\n",
    "print(len(test_df))\n",
    "print(len(test_df1))\n",
    "print(len(test_final_badcust_xgb))\n",
    "\n",
    "print(test_df1.columns)\n",
    "#print(test_df1.columns)\n",
    "\n",
    "test_merged = test_df.merge(test_df1[['Key', 'y_cb']], on = 'Key', how = 'left')\n",
    "\n",
    "#test_merged = test_merged.merge(test_final_lgb[['Key', 'y_lgb']], on = 'Key', how = 'left')\n",
    "\n",
    "# Combining with customer data\n",
    "test_merged = test_merged.merge(test_final_badcust_xgb[['Key', 'y_xgb']].rename(columns = {'y_xgb':'y_badcust'}),\\\n",
    "                                on = 'Key', how = 'left')\n",
    "\n",
    "test_merged['Comb_Price_with_bad_cust'] = np.where(test_merged['y_badcust'].isna(),\\\n",
    "                                    test_merged['y_cb'], test_merged['y_badcust'])\n",
    "\n",
    "test_merged['Comb_Price_with_org'] = np.where(((test_merged['NoOfUniquePricePoints'] == 1) &\\\n",
    "                                     (test_merged['DataAvailable'] > 100)),\\\n",
    "                                    test_merged['Price_Mode1'], test_merged['Comb_Price_with_bad_cust'])\n",
    "\n",
    "#test_merged['Comb_Price_with_bad_desc'] = np.where(test_merged['Description'] == 2140,\\\n",
    "#                                    test_merged['Manual_Value'], test_merged['Comb_Price_with_org'])\n",
    "\n",
    "#test_merged['Comb_Price_with_bad_desc'] = np.where(test_merged['Description'] == 2624,\\\n",
    "#                                    test_merged['y_merge'], test_merged['Comb_Price_with_bad_desc'])\n",
    "\n",
    "\n",
    "# Create Customer Mean Df to fill for IDS with No Description\n",
    "Customer_Mean_DF = train_df.groupby(['CustomerID'], as_index = False).agg(cust_mean_price= ('UnitPrice','mean'))\n",
    "test_merged = test_merged.merge(Customer_Mean_DF, on = 'CustomerID', how = 'left')\n",
    "\n",
    "\n",
    "test_merged['y_final1'] = np.where( test_merged['Comb_Price_with_bad_cust'].isna() ,\\\n",
    "                                    test_merged['cust_mean_price'], test_merged['Comb_Price_with_org'])\n",
    "\n",
    "\n",
    "# Join with Original Df at Description, Day Quantity to fill actual values if available\n",
    "\n",
    "left_on = ['InvoiceNo', 'StockCode', 'Description', 'Quantity_org', 'InvoiceDate_org', 'CustomerID', 'Country']\n",
    "right_on = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'CustomerID', 'Country']\n",
    "\n",
    "train_unique = train_df_org.groupby(right_on, as_index = False).agg(OrgPrice = ('UnitPrice', 'last'))\n",
    "#print(train_unique.head())\n",
    "\n",
    "test_merged = test_merged.merge(train_unique\n",
    "                                , left_on = left_on, right_on = right_on, how = 'left')\n",
    "\n",
    "#print(test_merged.columns)\n",
    "\n",
    "test_merged['y_final2'] = np.where( test_merged['OrgPrice'].isna() ,\\\n",
    "                                    test_merged['y_final1'], test_merged['OrgPrice'])\n",
    "\n",
    "\n",
    "#test_merged.to_csv('submission_type1.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change predictions to xgb_pred and lgb_pred to extract their outputs\n",
    "\n",
    "test_merged['UnitPrice'] = test_merged['y_final1'].apply(abs)\n",
    "test_merged[['UnitPrice']].to_csv('my_submission_file.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad Description\n",
    "\n",
    "desc_list = [2624,2140]\n",
    "#desc_list = [2140]\n",
    "\n",
    "score_df_badcust_xgb = pd.DataFrame()\n",
    "train_final_badcust_xgb = pd.DataFrame()\n",
    "test_final_badcust_xgb = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for country,train_d1 in train_df[train_df['Description'].isin( desc_list)].groupby(['Country']):\n",
    "    print('----------------------',country,'----------------------')\n",
    "    xgb = XGBRegressor(n_estimators = 100,\n",
    "    #                        learning_rate = 0.05,\n",
    "    #                       colsample_bytree = 0.76,\n",
    "                            )\n",
    "    \n",
    "    fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "    TARGET_COL= 'UnitPrice'\n",
    "    features = model_col\n",
    "    test_d1 = test_df[(test_df['Description'] == desc_list[i]) & (test_df['Country'] == country)]\n",
    "    split_val = min(len(train_d1),10)\n",
    "    \n",
    "    if len(test_d1)>0 and len(train_d1)>1:\n",
    "        xgb_oofs, xgb_preds, score = run_gradient_boosting(xgb, split_val, fit_params, train_d1, test_d1, features)\n",
    "        \n",
    "    elif len(train_d1) == 1:\n",
    "        un_val =  list(train_d1['UnitPrice'].unique())[0]\n",
    "        xgb_oofs, xgb_preds, score = np.zeros(len(train_d1)), np.repeat(un_val,len(test_d1)), 0\n",
    "        \n",
    "    else:\n",
    "        xgb_oofs, xgb_preds, score = np.zeros(len(train_d1)), np.zeros(len(test_d1)), 0\n",
    "        \n",
    "    temp_df = pd.DataFrame({'Description':[desc[1]], 'CustomerID':[desc[0]] ,'Price_Mode':[desc], 'xgb_score':[score],\\\n",
    "                            'Length_of_train':[len(train_d1)],\\\n",
    "                            'Length_of_test':[len(test_d1)],'unique_Desc':[train_d1['Description'].nunique()]})\n",
    "    \n",
    "    score_df_badcust_xgb = score_df_badcust_xgb.append(temp_df)\n",
    "    \n",
    "    # Adding in train data\n",
    "    # Adding in test data\n",
    "    train_d1['y_xgb'] = xgb_oofs\n",
    "    test_d1['y_xgb'] = xgb_preds\n",
    "    train_final_badcust_xgb = train_final_badcust_xgb.append(train_d1)\n",
    "    test_final_badcust_xgb = test_final_badcust_xgb.append(test_d1)\n",
    "    i += 1\n",
    "    #if i>=10:\n",
    "    #    break\n",
    "        \n",
    "\n",
    "score_df_badcust_xgb.to_csv('xgb_10_fold_score_bad_cust.csv', index = False)\n",
    "train_final_badcust_xgb.to_csv('xgb_10_fold_train_bad_cust.csv', index = False)\n",
    "test_final_badcust_xgb.to_csv('xgb_10_fold_test_bad_cust.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc_list = [2624]\n",
    "#desc_list = [2140]\n",
    "\n",
    "#train_df_desc = train_df[train_df['Description'].isin(desc_list)]\n",
    "#test_df_desc = test_df[test_df['Description'].isin(desc_list)]\n",
    "\n",
    "\n",
    "#sns.jointplot(x=train_df_desc['days_since_desc'], y=train_df_desc['UnitPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting For Desc 2624\n",
    "\n",
    "train_df_desc_2624['y_merge'] = 0.2*train_df_desc_2624['y_xgb'] + 0.8*train_df_desc_2624['y_cb']\n",
    "np.sqrt(mean_squared_error(train_df_desc_2624['UnitPrice'], train_df_desc_2624['y_merge']))\n",
    "\n",
    "test_df_desc_2624['y_merge'] = 0.2*train_df_desc_2624['y_xgb'] + 0.8*train_df_desc_2624['y_cb']\n",
    "\n",
    "test_df_desc_2624.at[((test_df_desc_2624['InvoiceNo'] == 5882 )\\\n",
    "                      &(test_df_desc_2624['days_since_desc'] == 153 ))\\\n",
    "                     ,'y_merge'] = 1\n",
    "\n",
    "test_df_desc_2624.at[((test_df_desc_2624['InvoiceNo'] == 5922 )\\\n",
    "                      &(test_df_desc_2624['days_since_desc'] == 153 ))\\\n",
    "                     ,'y_merge'] = 8142.75\n",
    "\n",
    "test_df_desc_2624.at[((test_df_desc_2624['InvoiceNo'] == 5876 )\\\n",
    "                      &(test_df_desc_2624['days_since_desc'] == 153 ))\\\n",
    "                     ,'y_merge'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Bad Customers and create Bad Customers Model seperate\n",
    "\n",
    "#train_df1\n",
    "#train_df2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Selecting Columns to take in the model\n",
    "model_col = [ 'CustomerID','Price_Mode_Sum',\\\n",
    "            'Price_Mean','Price_Median' ,'Price_Max', 'Price_Min', 'Price_StDev','Quantity',\\\n",
    "             'days_since_desc',\\\n",
    "           #'Quantity_Mean','Quantity_Median', 'Quantity_Max', 'Quantity_Min', 'Quantity_StDev', 'Quantity_Mode'\\\n",
    "            #'bad_customer_flag'\\\n",
    "            ]\n",
    "cat_col = ['Description', 'StockCode']\n",
    "tar_col = ['UnitPrice']\n",
    "\n",
    "xgb = XGBRegressor(n_estimators = 1000,\n",
    "#                        learning_rate = 0.05,\n",
    "#                       colsample_bytree = 0.76,\n",
    "                        )\n",
    "fit_params = {'verbose': False, 'early_stopping_rounds': 100}\n",
    "TARGET_COL= 'UnitPrice'\n",
    "features = model_col\n",
    "xgb_oofs, xgb_preds,score_xgb = run_gradient_boosting(xgb,10, fit_params, train_df2, test_df2, features)\n",
    "\n",
    "train_df2['y_xgb'] = xgb_oofs\n",
    "test_df2['y_xgb'] = xgb_preds\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Data For Bad Description 2140 and 2624\n",
    "\n",
    "test_desc_2140 = pd.read_csv('Manual_Entry.csv', parse_dates = ['InvoiceDate'])\n",
    "test_desc_2140['CustomerID'] = test_desc_2140['CustomerID'].apply(lambda x:str(int(float(x))))\n",
    "\n",
    "#test_desc_2140.head()\n",
    "#test_df.head()\n",
    "\n",
    "#print(test_desc_2140.columns)\n",
    "#print(test_df.columns)\n",
    "\n",
    "# Manual_Value\n",
    "left_on = ['InvoiceNo', 'StockCode', 'Description', 'Quantity_org', 'InvoiceDate_org', 'CustomerID', 'Country']\n",
    "right_on = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'CustomerID', 'Country']\n",
    "\n",
    "test_merged = test_df.merge(test_desc_2140, left_on = left_on, right_on = right_on, how = 'left')\n",
    "\n",
    "# For 2624\n",
    "test_merged = test_merged.merge(test_df_desc_2624[['Key', 'y_merge']], on = 'Key', how = 'left')\n",
    "\n",
    "test_merged.to_csv('Test_Merged.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Price Point and No Of Samples >=10 then override with Mode Price for all test values\n",
    "\n",
    "'NoOfOrders', 'CountOfDistinctDate', 'NoOfUniquePricePoints'\n",
    "\n",
    "print(len(test_df))\n",
    "print(len(test_df1))\n",
    "print(len(test_final_badcust_xgb))\n",
    "\n",
    "print(test_df1.columns)\n",
    "#print(test_df1.columns)\n",
    "\n",
    "#test_df1['y_merge'] = 0.45*test_df1['y_xgb'] + 0.1*test_df1['y_xgb'] + 0.45*test_df1['y_xgb']\n",
    "\n",
    "test_merged = test_merged.merge(test_df1[['Key', 'l_xgb']], on = 'Key', how = 'left')\n",
    "\n",
    "#test_merged = test_merged.merge(test_final_lgb[['Key', 'y_lgb']], on = 'Key', how = 'left')\n",
    "\n",
    "# Combining with customer data\n",
    "test_merged = test_merged.merge(test_final_badcust_xgb[['Key', 'y_xgb']].rename(columns = {'y_xgb':'y_badcust'}),\\\n",
    "                                on = 'Key', how = 'left')\n",
    "\n",
    "test_merged['Comb_Price_with_bad_cust'] = np.where(test_merged['y_badcust'].isna(),\\\n",
    "                                    test_merged['y_xgb'], test_merged['y_badcust'])\n",
    "\n",
    "test_merged['Comb_Price_with_bad_desc'] = np.where(test_merged['Description'] == 2140,\\\n",
    "                                    test_merged['Manual_Value'], test_merged['Comb_Price_with_bad_cust'])\n",
    "\n",
    "test_merged['Comb_Price_with_bad_desc'] = np.where(test_merged['Description'] == 2624,\\\n",
    "                                    test_merged['y_merge'], test_merged['Comb_Price_with_bad_desc'])\n",
    "\n",
    "\n",
    "# Create Customer Mean Df to fill for IDS with No Description\n",
    "Customer_Mean_DF = train_df.groupby(['CustomerID'], as_index = False).agg(cust_mean_price= ('UnitPrice','mean'))\n",
    "test_merged = test_merged.merge(Customer_Mean_DF, on = 'CustomerID', how = 'left')\n",
    "\n",
    "\n",
    "test_merged['y_final1'] = np.where( test_merged['Comb_Price_with_bad_desc'].isna() ,\\\n",
    "                                    test_merged['cust_mean_price'], test_merged['Comb_Price_with_bad_desc'])\n",
    "\n",
    "# Join with Original Df at Description, Day Quantity to fill actual values if available\n",
    "\n",
    "left_on = ['InvoiceNo', 'StockCode', 'Description', 'Quantity_org', 'InvoiceDate_org', 'CustomerID', 'Country']\n",
    "right_on = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'CustomerID', 'Country']\n",
    "\n",
    "train_unique = train_df_org.groupby(right_on, as_index = False).agg(OrgPrice = ('UnitPrice', 'mean'))\n",
    "print(train_unique.head())\n",
    "\n",
    "test_merged = test_merged.merge(train_unique\n",
    "                                , left_on = left_on, right_on = right_on, how = 'left')\n",
    "\n",
    "print(test_merged.columns)\n",
    "\n",
    "test_merged['y_final2'] = np.where( test_merged['OrgPrice'].isna() ,\\\n",
    "                                    test_merged['y_final1'], test_merged['OrgPrice'])\n",
    "\n",
    "\n",
    "test_merged.to_csv('submission_type1.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change predictions to xgb_pred and lgb_pred to extract their outputs\n",
    "\n",
    "test_merged['UnitPrice'] = test_merged['y_final1'].apply(abs)\n",
    "test_merged[['UnitPrice']].to_csv('my_submission_file.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
